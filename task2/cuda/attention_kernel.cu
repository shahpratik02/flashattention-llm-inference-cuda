#include <cuda_runtime.h>
#include <torch/extension.h>
#include <float.h>
#include <math.h>

using namespace torch::indexing;

__global__ void softmax_kernel_batched(float *inp, float *outp, int NUM_ROW, int NUM_COL)
{
    extern __shared__ float buffer[];

    // ########################################################

    // TODO: Implement the softmax operation 

    // ########################################################
}

template <int TILE_SIZE>
__global__ void GEMM_NT_kernel_batched(float *a_mat, float *b_mat, float *out_mat, int M, int N, int K)
{
    __shared__ float a_tile[TILE_SIZE][TILE_SIZE];
    __shared__ float b_tile[TILE_SIZE][TILE_SIZE];

    // ########################################################

    // TODO: Implement the NT GEMM operation 

    // ########################################################
}

template <int TILE_SIZE>
__global__ void GEMM_NN_kernel_batched(float *a_mat, float *b_mat, float *out_mat, int M, int N, int K)
{
    __shared__ float a_tile[TILE_SIZE][TILE_SIZE];
    __shared__ float b_tile[TILE_SIZE][TILE_SIZE];

    // ########################################################

    // TODO: Implement the NN GEMM operation 

    // ########################################################
}

__global__ void scale_and_causal_mask_batched(float *mat, int rows, int cols, float scale, bool causal)
{
    int b = blockIdx.x;
    int row = blockIdx.y;
    int tid = threadIdx.x;
    if (row >= rows) return;

    int stride = rows * cols;
    float *row_ptr = mat + b * stride + row * cols;
    for (int j = tid; j < cols; j += blockDim.x)
    {
        float val = row_ptr[j] * scale;
        if (causal && j > row)
        {
            val = -FLT_MAX;
        }
        row_ptr[j] = val;
    }
}

torch::Tensor custom_attention(torch::Tensor q, torch::Tensor k, torch::Tensor v, int num_heads, bool causal)
{
    auto options = q.options();

    const int TILE_SIZE = 16;

    int batch_size = q.size(0);
    int seq_len = q.size(1);
    int hidden_dim = q.size(2);
    int head_dim = hidden_dim / num_heads;

    // Reshape to (batch_size * num_heads, seq_len, head_dim) for parallel per-head processing
    auto q_bh = q.view({batch_size, seq_len, num_heads, head_dim}).permute({0, 2, 1, 3}).contiguous().view({batch_size * num_heads, seq_len, head_dim});
    auto k_bh = k.view({batch_size, seq_len, num_heads, head_dim}).permute({0, 2, 1, 3}).contiguous().view({batch_size * num_heads, seq_len, head_dim});
    auto v_bh = v.view({batch_size, seq_len, num_heads, head_dim}).permute({0, 2, 1, 3}).contiguous().view({batch_size * num_heads, seq_len, head_dim});

    auto qk = torch::empty({batch_size * num_heads, seq_len, seq_len}, options);
    auto s = torch::empty_like(qk);
    auto o_bh = torch::empty({batch_size * num_heads, seq_len, head_dim}, options);

    // ########################################################

    // TODO: Run QK^T batched
                
    // TODO: Run Scale + causal mask batched (over BH, rows)
    
    // TODO: Run Softmax batched along last dim
    
    // TODO: Run (S @ V) batched
    
    // ########################################################
    
    // Reshape back to (batch_size, seq_len, hidden_dim)
    auto o = o_bh.view({batch_size, num_heads, seq_len, head_dim}).permute({0, 2, 1, 3}).contiguous().view({batch_size, seq_len, hidden_dim});
    return o;
}

torch::Tensor test_batched_softmax(torch::Tensor inp, int dim) {
    int N_DIM = inp.dim();
    if (dim < 0) {
        dim += N_DIM;
    }
    if (dim != N_DIM - 1) {
        inp = inp.transpose(dim, N_DIM - 1);
    }
    // Ensure contiguous memory layout for linear indexing in the CUDA kernel
    inp = inp.contiguous();

    auto inp_sizes = inp.sizes();
    int N_BATCH = inp_sizes[0];
    int N_ROW = inp_sizes[1];
    int N_COL = inp_sizes[2];
    dim3 threads_per_block(256);
    dim3 blocks_per_grid(N_BATCH, N_ROW);
    auto outp = torch::empty(inp_sizes, inp.options());
    
    // Allocate shared memory for each thread in the block
    softmax_kernel_batched<<<blocks_per_grid, threads_per_block, threads_per_block.x * sizeof(float)>>>(
        inp.data_ptr<float>(), outp.data_ptr<float>(), N_ROW, N_COL);
    
    if (dim != N_DIM - 1) {
        inp = inp.transpose(dim, N_DIM - 1);
        outp = outp.transpose(dim, N_DIM - 1);
    }

    return outp;
}

torch::Tensor test_batched_GEMM_NN(torch::Tensor a, torch::Tensor b)
{
    auto a_size = a.sizes();
    auto b_size = b.sizes();
    int N_BATCH = a_size[0];
    int M = a_size[1], N = b_size[2], K = a_size[2];

    auto out = torch::empty({N_BATCH, M, N}, a.options());
    const int TILE_SIZE = 16;
    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);
    dim3 blocks_per_grid(
        (N + TILE_SIZE - 1) / TILE_SIZE,
        (M + TILE_SIZE - 1) / TILE_SIZE,
        N_BATCH);
    GEMM_NN_kernel_batched<TILE_SIZE><<<blocks_per_grid, threads_per_block>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), M, N, K);
    return out;
}

torch::Tensor test_batched_GEMM_NT(torch::Tensor a, torch::Tensor b)
{
    auto a_size = a.sizes();
    auto b_size = b.sizes();
    int N_BATCH = a_size[0];
    int M = a_size[1], N = b_size[1], K = a_size[2];

    auto out = torch::empty({N_BATCH, M, N}, a.options());
    const int TILE_SIZE = 16;
    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);
    dim3 blocks_per_grid(
        (N + TILE_SIZE - 1) / TILE_SIZE,
        (M + TILE_SIZE - 1) / TILE_SIZE,
        N_BATCH);
    GEMM_NT_kernel_batched<TILE_SIZE><<<blocks_per_grid, threads_per_block>>>(a.data_ptr<float>(), b.data_ptr<float>(), out.data_ptr<float>(), M, N, K);
    return out;
}

// Python bindings
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("test_batched_softmax", &test_batched_softmax, "Testing interface for custom softmax in CUDA");
    m.def("test_batched_GEMM_NN", &test_batched_GEMM_NN, "Testing interface for custom matmul (A @ B) in CUDA");
    m.def("test_batched_GEMM_NT", &test_batched_GEMM_NT, "Testing interface for custom matmul (A @ B^T) in CUDA");
    m.def("custom_attention", &custom_attention, "Custom attention in CUDA");
}