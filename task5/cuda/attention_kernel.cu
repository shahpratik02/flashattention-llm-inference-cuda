#include <cuda_runtime.h>
#include <torch/extension.h>
#include <float.h>
#include <math.h>

using namespace torch::indexing;

// FlashAttention kernel
template <int B>
__global__ void flash_attention_decode_kernel(
    const float* __restrict__ q,
    const float* __restrict__ k,
    const float* __restrict__ v,
    float* __restrict__ out,
    int seq_len,
    int head_dim,
    bool causal)
{
    // Shared memory for tiles
    __shared__ float q_tile[128];  // Max head_dim = 128
    __shared__ float k_tile[B][128];
    __shared__ float v_tile[B][128];
    __shared__ float o_tile[128];
    __shared__ float m_tile;
    __shared__ float l_tile;
    __shared__ float scores[B];
    __shared__ float p[B];

    // ########################################################

    // TODO: Implement the FlashAttention operation 

    // ########################################################
}

torch::Tensor custom_flash_attention_decode(torch::Tensor q, torch::Tensor k, torch::Tensor v, int num_heads, bool causal) {
    auto options = q.options();
    
    int batch_size = q.size(0);
    int seq_len = k.size(1);
    int hidden_dim = q.size(2);
    int head_dim = hidden_dim / num_heads;
    
    // Reshape to (batch_size * num_heads, 1, head_dim)
    auto q_bh = q.view({batch_size, 1, num_heads, head_dim})
                 .permute({0, 2, 1, 3})
                 .contiguous()
                 .view({batch_size * num_heads, 1, head_dim});
    auto k_bh = k.view({batch_size, seq_len, num_heads, head_dim})
                 .permute({0, 2, 1, 3})
                 .contiguous()
                 .view({batch_size * num_heads, seq_len, head_dim});
    auto v_bh = v.view({batch_size, seq_len, num_heads, head_dim})
                 .permute({0, 2, 1, 3})
                 .contiguous()
                 .view({batch_size * num_heads, seq_len, head_dim});
    
    auto out_bh = torch::zeros_like(q_bh, options);
    
    // ########################################################

    // TODO: launch the kernel
    
    // ########################################################

    // Check for errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("CUDA Error: %s\n", cudaGetErrorString(err));
    }
    
    // Reshape back to (batch_size, 1, hidden_dim)
    auto out = out_bh.view({batch_size, num_heads, 1, head_dim})
                     .permute({0, 2, 1, 3})
                     .contiguous()
                     .view({batch_size, 1, hidden_dim});
    
    return out;
}

__global__ void update_cache_kernel(
    float* __restrict__ k_cache,
    float* __restrict__ v_cache,
    const float* __restrict__ k,
    const float* __restrict__ v,
    int batch_size,
    int current_pos,
    int max_seq_len,
    int hidden_dim
) {
    // ########################################################

    // TODO: Implement the update kv cache operation 

    // ########################################################
}

void update_kv_cache(torch::Tensor k_cache, torch::Tensor v_cache, torch::Tensor k, torch::Tensor v, int current_pos) {
    auto options = k_cache.options();

    int batch_size = k_cache.size(0);
    int max_seq_len = k_cache.size(1);
    int hidden_dim = k_cache.size(2);
    
    int total_threads = batch_size * hidden_dim;

    int threads_per_block = 256;
    dim3 threads(threads_per_block);
    dim3 blocks((total_threads + threads_per_block - 1) / threads_per_block);

    update_cache_kernel<<<blocks, threads>>>(
        k_cache.data_ptr<float>(),
        v_cache.data_ptr<float>(),
        k.data_ptr<float>(),
        v.data_ptr<float>(),
        batch_size,
        current_pos,
        max_seq_len,
        hidden_dim
    );
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        printf("CUDA Error: %s\n", cudaGetErrorString(err));
    }
}

// Python bindings
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("custom_flash_attention_decode", &custom_flash_attention_decode, "Custom FlashAttention in CUDA");
    m.def("update_kv_cache", &update_kv_cache, "Update KV cache in CUDA");
}
